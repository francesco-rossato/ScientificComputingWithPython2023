{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. **PCA on 3D dataset**\n",
    "\n",
    "* Generate a dataset simulating 3 features, each with N entries (N being ${\\cal O}(1000)$). Each feature is made by random numbers generated according the normal distribution $N(\\mu,\\sigma)$ with mean $\\mu_i$ and standard deviation $\\sigma_i$, with $i=1, 2, 3$. Generate the 3 variables $x_{i}$ such that:\n",
    "    * $x_1$ is distributed as $N(0,1)$\n",
    "    * $x_2$ is distributed as $x_1+N(0,3)$\n",
    "    * $x_3$ is given by $2x_1+x_2$\n",
    "* Find the eigenvectors and eigenvalues using the eigendecomposition of the covariance matrix\n",
    "* Find the eigenvectors and eigenvalues using the SVD. Check that the two procedures yield to same result\n",
    "* What percent of the total dataset's variability is explained by the principal components? Given how the dataset was constructed, do these make sense? Reduce the dimensionality of the system so that at least 99% of the total variability is retained\n",
    "* Redefine the data according to the new basis from the PCA\n",
    "* Plot the data, in both the original and the new basis. The figure should have 2 rows (the original and the new basis) and 3 columns (the $[x_0, x_1]$, $[x_0, x_2]$ and $[x_1, x_2]$ projections) of scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from scipy import linalg as la\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results using the eigendecomposition of the covariance matrix\n",
      "\n",
      "real(eigenvalues):\n",
      " [2.39768746e+01 3.61431738e-16 2.09776265e+00] \n",
      "\n",
      "eigenvectors:\n",
      " [[-0.11924782 -0.81649658  0.56490113]\n",
      " [-0.57261194 -0.40824829 -0.71094929]\n",
      " [-0.81110759  0.40824829  0.41885297]] \n",
      "\n",
      "results using the single value decomposition of the covariance matrix\n",
      "\n",
      "eigenvalues:\n",
      " [2.39768746e+01 2.09776265e+00 5.96705218e-16] \n",
      "\n",
      "eigenvectors:\n",
      " [[-0.11924782  0.56490113 -0.81649658]\n",
      " [-0.57261194 -0.71094929 -0.40824829]\n",
      " [-0.81110759  0.41885297  0.40824829]] \n",
      "\n",
      "note that the second and third eigenvalue (and consequently their eigenvectors) are swapped between the two methods. This is not a problem, but if we want to compare the two methods, we need to swap their values.\n",
      "the swapped eigenvector matrix reads\n",
      "\n",
      "[[-0.11924782 -0.81649658  0.56490113]\n",
      " [-0.57261194 -0.40824829 -0.71094929]\n",
      " [-0.81110759  0.40824829  0.41885297]]\n",
      "\n",
      "and now the method 'allclose' returns True\n",
      "total variability explained by each component: [0.9195477723572134, 0.08045222764278667, 2.2884506975932476e-17]\n",
      "since the third column is a linear combination of the first two, it makes sense that the last components bears a very small variability.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "size = 500\n",
    "x1 = npr.normal(loc=0, scale=1, size=size)\n",
    "x2 = x1 + npr.normal(loc=0, scale=3, size=size)\n",
    "x3 = 2*x1 + x2\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'x1' : x1.T,\n",
    "    'x2' : x2.T,\n",
    "    'x3' : x3.T,\n",
    "})\n",
    "\n",
    "# display(df)\n",
    "\n",
    "\n",
    "# calculate the covariance matrix\n",
    "cov = np.cov(df, rowvar = False)\n",
    "\n",
    "# find the eigenvalues using the eigendecomposition of the covariance matrix\n",
    "eValues1, eVectors1 = la.eig(cov)\n",
    "print(\"results using the eigendecomposition of the covariance matrix\\n\")\n",
    "print(\"real(eigenvalues):\\n\", np.real_if_close(eValues1), '\\n')\n",
    "# printing the eigenvectors\n",
    "print(\"eigenvectors:\\n\", eVectors1, '\\n')\n",
    "\n",
    "# find the eigenvalues using the SVD of the matrix\n",
    "eVectors2, eValues2, _ = la.svd(cov)\n",
    "print(\"results using the single value decomposition of the covariance matrix\\n\")\n",
    "print(\"eigenvalues:\\n\", eValues2, '\\n')\n",
    "print(\"eigenvectors:\\n\", eVectors2, '\\n')\n",
    "# the eigenvectors and eigenvalues are swapped\n",
    "print(\"note that the second and third eigenvalue (and consequently their eigenvectors)\",\n",
    "      \"are swapped between the two methods. This is not a problem, but if we want to\",\n",
    "      \"compare the two methods, we need to swap their values.\")\n",
    "eVectors2[0][2], eVectors2[0][1] = eVectors2[0][1], eVectors2[0][2]\n",
    "eVectors2[1][2], eVectors2[1][1] = eVectors2[1][1], eVectors2[1][2]\n",
    "eVectors2[2][2], eVectors2[2][1] = eVectors2[2][1], eVectors2[2][2]\n",
    "print(\"the swapped eigenvector matrix reads\\n\")\n",
    "print(eVectors2)\n",
    "print(\"\\nand now the method 'allclose' returns\", np.allclose(eVectors1, eVectors2))\n",
    "\n",
    "\n",
    "# part 2: principal component analysis\n",
    "variability = sum(eValues2)\n",
    "fractionVariability = [(i / variability) for i in sorted(eValues2, reverse=True)]\n",
    "\n",
    "# Print the results\n",
    "print('total variability explained by each component:', fractionVariability)\n",
    "print(\"since the third column is a linear combination of the first two,\",\n",
    "      \"it makes sense that the last components bears a very small variability.\\n\")\n",
    "\n",
    "# select the first two principal components\n",
    "pc = eVectors2[:, :2]\n",
    "\n",
    "# Project the data onto the first two principal components\n",
    "projected_data = np.dot(df, pc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. **PCA on a nD dataset**\n",
    "\n",
    "* Start from the dataset you have genereted in the previous exercise and add uncorrelated random noise. Such noise should be represented by other 10 uncorrelated variables normally distributed, with a standard deviation much smaller (e.g. a factor 20) than those used to generate the $x_1$ and $x_2$. Repeat the PCA procedure and compare the results with what you have obtained before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. **Optional**: **PCA on the MAGIC dataset**\n",
    "\n",
    "Perform a PCA on the magic04.data dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset and its description on the proper data directory\n",
    "#!wget https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data -P data/\n",
    "#!wget https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.names -P data/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
